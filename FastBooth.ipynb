{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU7NuMAA2drw",
    "outputId": "7eb9b063-664f-4a42-e960-728ec9608c42"
   },
   "outputs": [],
   "source": [
    "#@markdown Check type of GPU and VRAM available.\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnTMyW41cC1E"
   },
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip #pip is outdated for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLWXPZqjsZVV"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
    "%pip install -q -U --pre triton\n",
    "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ðŸ¤— api read only key (https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "y4lqqWT_uxD2"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.huggingface\n",
    "HUGGINGFACE_TOKEN = \"token-goes-here\" #@param {type:\"string\"}\n",
    "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfTlc8Mqb8iH"
   },
   "source": [
    "## Install xformers from precompiled wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits -> https://github.com/daswer123/xformers_prebuild_wheels\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "#Function for determining compute capability \n",
    "def get_compute_capability(string):\n",
    "    return string.split(',')[3].split(':')[1].strip()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "gpu = device_lib.list_local_devices()\n",
    "compute = get_compute_capability(gpu[1].physical_device_desc)\n",
    "\n",
    "print(gpu[1].physical_device_desc)\n",
    "print(compute)\n",
    "\n",
    "#Install precompiled wheel\n",
    "if(compute == \"8.6\"):\n",
    "    install(\"https://github.com/daswer123/xformers_prebuild_wheels/raw/main/sm_86/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\")\n",
    "if(compute == \"7.5\"):\n",
    "    install(\"https://github.com/daswer123/xformers_prebuild_wheels/raw/main/sm_75/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\")\n",
    "if(compute == \"6.1\"):\n",
    "    install(\"https://github.com/daswer123/xformers_prebuild_wheels/raw/main/sm_61/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\")\n",
    "if(compute == \"5.2\"):\n",
    "    install(\"https://github.com/daswer123/xformers_prebuild_wheels/raw/main/sm_52/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install triton --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m xformers.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0NV324ZcL9L"
   },
   "source": [
    "## Settings and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rxg0y5MBudmd"
   },
   "outputs": [],
   "source": [
    "# Name of the source model from hf model's hub\n",
    "MODEL_NAME = \"dreamlike-art/dreamlike-diffusion-1.0\"\n",
    "\n",
    "OUTPUT_DIR = \"models/output/\"\n",
    "OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
    "!mkdir -p $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vDpCxId1aCm"
   },
   "outputs": [],
   "source": [
    "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      \"photo of zwx dog\",\n",
    "        \"class_prompt\":         \"photo of a dog\",\n",
    "        \"instance_data_dir\":    \"data/zwx\",\n",
    "        \"class_data_dir\":       \"data/dog\"\n",
    "    },\n",
    "#     {\n",
    "#         \"instance_prompt\":      \"photo of ukj person\",\n",
    "#         \"class_prompt\":         \"photo of a person\",\n",
    "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
    "#         \"class_data_dir\":       \"/content/data/person\"\n",
    "#     }\n",
    "]\n",
    "\n",
    "# `class_data_dir` contains regularization images\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn5ILIyDJIcX"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ioxxvHoicPs"
   },
   "source": [
    "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
    "\n",
    "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
    "\n",
    "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjcSXTp-u-Eg"
   },
   "outputs": [],
   "source": [
    "!accelerate launch train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --use_8bit_adam \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --num_class_images=50 \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=1000 \\\n",
    "  --save_interval=10000 \\\n",
    "  --save_sample_prompt=\"photo of zwx dog\" \\\n",
    "  --concepts_list=\"concepts_list.json\"\n",
    "\n",
    "\n",
    "# Add --revision=\"fp16\" if the source model has an fp16 version\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "#Specify the weights directory to use (leave blank for latest)\n",
    "WEIGHTS_DIR = \"\"\n",
    "if WEIGHTS_DIR == \"\":\n",
    "    from natsort import natsorted\n",
    "    from glob import glob\n",
    "    import os\n",
    "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "89Az5NUxOWdy"
   },
   "outputs": [],
   "source": [
    "# Run to generate a grid of preview images from the last saved weights.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap='gray')\n",
    "        currAxes.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('grid.png', dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8wgU0HN-Kq"
   },
   "source": [
    "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dcXzsUyG1aCy"
   },
   "outputs": [],
   "source": [
    "#Run conversion.\n",
    "ckpt_path =  WEIGHTS_DIR + \"/model.ckpt\"\n",
    "\n",
    "half_arg = \"\"\n",
    "#Whether to convert to fp16, takes half the space (2GB).\n",
    "fp16 = True\n",
    "if fp16:\n",
    "    half_arg = \"--half\"\n",
    "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning! Only run if the vram usage is still high after finishing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToNG4fd_dTbF"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW15FjffdTID"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "model_path =  WEIGHTS_DIR     \n",
    "\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "g_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIzkltjpVO_f",
    "outputId": "1db9fcaa-2d0f-4966-dc4f-baac60cdb807"
   },
   "outputs": [],
   "source": [
    "# Set random seed here for reproducibility.\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "seed = 396159304\n",
    "g_cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "referenced_widgets": [
      "21d3153693b0442bb0afe46738c9d9ae"
     ]
    },
    "id": "K6xoHWSsbcS3",
    "outputId": "75fb2672-a0a4-4149-ef0d-42ff0c247449"
   },
   "outputs": [],
   "source": [
    "#Run for generating images.\n",
    "\n",
    "prompt = \"zwx dog oil painting, dreamlikeart\"\n",
    "negative_prompt = \"\"\n",
    "num_samples = 5 \n",
    "guidance_scale = 5\n",
    "num_inference_steps = 50 \n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "with autocast(\"cuda\"), torch.inference_mode():\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=g_cuda\n",
    "    ).images\n",
    "\n",
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WMCqQ5Tcdsm2"
   },
   "outputs": [],
   "source": [
    "#@markdown Run Gradio UI for generating images.\n",
    "import gradio as gr\n",
    "\n",
    "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
    "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "        return pipe(\n",
    "                prompt, height=int(height), width=int(width),\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_images_per_prompt=int(num_samples),\n",
    "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                generator=g_cuda\n",
    "            ).images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
    "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
    "            run = gr.Button(value=\"Generate\")\n",
    "            with gr.Row():\n",
    "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
    "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
    "            with gr.Row():\n",
    "                height = gr.Number(label=\"Height\", value=512)\n",
    "                width = gr.Number(label=\"Width\", value=512)\n",
    "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
    "        with gr.Column():\n",
    "            gallery = gr.Gallery()\n",
    "\n",
    "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.  \n",
    "Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellView": "form",
    "id": "lJoOgLQHnC8L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted models/output/0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from glob import glob\n",
    "import os\n",
    "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
    "    if f != WEIGHTS_DIR:\n",
    "        shutil.rmtree(f)\n",
    "        print(\"Deleted\", f)\n",
    "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
    "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
    "        try:\n",
    "            shutil.rmtree(f)\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "        print(\"Deleted\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model to a huggingface repo\n",
    "Enter the write-api key you got from https://huggingface.co/settings/tokens\n",
    "Alternatively use huggingface-cli login in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=WEIGHTS_DIR,\n",
    "    path_in_repo=\"/\",\n",
    "    repo_id=\"hf_username/repo_name\",\n",
    "    repo_type=\"model\",\n",
    "    ignore_patterns=\"**/logs/*.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscale the images uses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/xinntao/Real-ESRGAN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd Real-ESRGAN\n",
    "!pip install basicsr\n",
    "# facexlib and gfpgan are for face enhancement\n",
    "!pip install facexlib\n",
    "!pip install gfpgan\n",
    "!pip install -r requirements.txt\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_folder = 'upload'\n",
    "result_folder = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference_realesrgan.py -n RealESRGAN_x4plus -i ../upload --outscale 3.5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
